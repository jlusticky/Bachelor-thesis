%=========================================================================
% (c) 2011, 2012 Josef Lusticky

\section{Contiki NTP client}\label{sec:design-client}
The client application itself is a Contiki process,
which will use the designed operating system interface from the previous sections
and the uIP communication stack.

The client should be able to use both NTP communication modes,
the NTP broadcast mode and the NTP unicast mode.
The NTP broadcast communication mode is intended particularly for energy or
even more memory constrained clients.
If the client will use only the broadcast mode, the structures and code
related to the unicast mode should not be included in the resulting program.

%TODO

As discussed in section~\ref{sec:analysis-application}, the NTP broadcast mode requires no server
associations.
A simple packet receiving is not a matter for constrained systems.
 %% CHECK !! The NTP broadcast mode packet can be received and processed from any NTP server in the network.


In NTP unicast mode, if the NTP client support towards more NTP servers,
the intersection algorithm, described in section~\ref{sec:ntp-algorithms}, would be needed.

In NTP unicast mode, the server associations are needed in the client.
This would complicate the client design and 
Support for more servers would also need the algorithms described in section~\ref{sec:ntp-algorithms}.

Section described, a network with
and the algorithm is complicated.
A single NTP server is the most common use case for a Simple Network Time Protocol client.

A simple NTP client (SNTP client) has a single NTP server~\cite{rfc5905}.


but it can send the request to only one specified NTP server.




%TODO - Future work:
%It is useful to provide an initial volley where the client operating in
%client mode exchanges several packets with the server, so as to
%calibrate the propagation delay and to run the Autokey security
%protocol, after which the client reverts to broadcast client mode~\cite{rfc5905}.



The NTP client fills and checks only the seconds part of the NTP timestamp,
because the conversion to the NTP format would increase the interval
between the timestamp determination and the dispatch of the filled packet.

After the filled NTP packet is sent, the client schedules
the dispatch of the next NTP packet in $2^{\tau}$ seconds
using the event timer library.
In NTPv4, $\tau$ ranges from 4, resulting in NTP poll interval of 16 seconds,
to 17, resulting in NTP poll interval of 36 hours.
However, the event timer library imposes a limit on the scheduled time.
This limit is platform specific and depends on the {\it{CLOCK\_SECOND}} value,
e.g. the $\tau$ value can not be greater than 8 on AVR Raven assuming 128 interrupts per second.
Upon scheduling the event timer, the client process yields
and another process can be run.
The client process is later invoked either by the uIP stack event
announcing the server response
or by the event timer in case no server response arrived.
The event timer therefore effectively solves
the possible packet loss problem described in section~\ref{sec:analysis-application}.


%%
The packet loss problem was described in section~\ref{sec:analysis-application}.
However, packet loss is not a matter for NTP if using either broadcast or unicast mode.
In broadcast mode, a lost server packet causes no setting or adjusting the client's system time.
The client simply waits without disruption for the next NTP broadcast message.
If the client needs to figure out it's local clock offset at the moment,
it can simply query the server using the NTP unicast mode.

%%%TODO


When the server response arrives,
the destination timestamp determination is one of the first tasks the client does.
After that, the client makes packet sanity tests, including
checking whether the response is from the synchronised server.

A determination of the NTP communication mode follows.
In the unicast mode, the Originate timestamp is compared with the stored sent timestamp.

The received packet is considered bogus in case of mismatch and further processing is stopped.
Otherwise, the NTP timestamps are converted to the local timestamp format and
the local clock offset is computed as described in section~\ref{sec:ntp-algorithms}.
After the local clock offset is computed,
the stored transmitted timestamp is immediately set to zero
to protect against a replay of the last transmitted packet.

In broadcast mode, the received packet is always considered correct
and the local clock offset is computed as the difference between the local stored timestamp
and the received Transmit timestamp.
The local clock offset determined from the broadcast mode
is influenced by the network propagation delay and therefore less accurate.

The NTP client could exchange several packets with the server to calibrate the propagation delay.
But since local variables can not be reused in the Contiki process when the process yields,
this would cause either an extra memory overhead or a complicate client design.





%DONE
Section~\ref{sec:design-interface} described that the client uses the POSIX timescale,
whereas NTP uses the NTP timescale.
Because the time is reckoned in seconds by both timescales,
the number of seconds between the NTP epoch and the POSIX epoch
can be simply subtracted from the seconds part of the NTP timestamp.
But the conversion from the fraction part of the long 64-bit NTP timestamp to nanoseconds,
used in the local timestamp structure,
is one of the most problematic tasks for memory constrained systems.
An accurate conversion requires either floating point operations or operations including 64-bit numbers~\cite{c99}.
The conversion is given by
$nsec = fractionl \times 10^9 \div 2^{32}$, where $nsec$ is the nanoseconds part of the local timestamp
and $fractionl$ is the fraction part of the long 64-bit NTP timestamp.

Since there is no native hardware support for floating point nor 64-bit arithmetic operations,
the compiler supplies these operations in form of library, called {\it{libgcc}} in case of the GCC compiler,
which causes a significantly bigger resulting binary file
(kilobytes in case of floating point operations and hundreds of bytes in case of 64-bit operations).
The greatest common divisor of $10^9$ and $2^{32}$ is $2^9$,
so in fact, a relatively simple multiplication of $fractionl$ by $\frac{5^9}{2^{23}}$ must be computed.
This could be computed on 32 bits using sequential
divisions by the power of 2 and multiplications by the power of 5.
In the standard C programming language, the bitwise right shift operator divides the unsigned data type by the power of 2
and the bitwise left shift operator multiplies the unsigned data type by the power of 2~\cite{c99}.
Therefore, the multiplication by 5 can be done using two left shifts and
adding the original value ($5x = 4x + x$).
The only constraint is that the overall coefficient of these operations must not be greater than 1,
that is, the value must be in the range from $0$ to $2^{32}-1$ in every step.
Otherwise, the value could overflow and the result would be incorrect.
Division can not cause such a situation but multiplication could.
The original value could be divided by a greater divisor,
but this would lead to a greater inaccuracy due to the lost of the least significant bits.
Because of this, multiplication done as soon as possible provides more accurate results.
The ideal conversion sequence is therefore given by formula~\ref{equ:conversion}.
\begin{equation}
\label{equ:conversion}
%\frac{10^9}{2^{32}} = \frac{2^9 \times 5^9}{2^9 \times 2^{23}} =
%\frac{5^9}{2^{23}} =
nsec = fractionl \times \frac{5}{2^3} \times \frac{5}{2^3} \times \frac{5}{2^3} \times \frac{5^2}{2^3} \times \frac{5}{2^3}  \times \frac{5}{2^3} \times \frac{5^2}{2^3} \times \frac{1}{2^2}
\end{equation}
It must be noted, that the above presented conversion is not exactly accurate,
because the least significant bits are lost by right shifts. %!LYDIA by
The accuracy can be determined by looping through all the possible values of $fractionl$ %!LYDIA by
and comparing the results with the reference algorithm that uses the floating point operations.
Such a measurement reports the maximum error of 5 nanoseconds,
which is totally adequate for most platforms without the floating point unit or
for platforms where 64-bit multiplication is expensive.
The implementation of the above as well as the program used for the
error measurement can be found on the CD attached to this thesis.
The table of CD contents is listed in appendix~\ref{app:cd-contents}.


After the timestamps were converted, the local clock offset is computed
as given in section~\ref{sec:ntp-algorithms}.
Depending on the absolute value of the local clock offset,
the system time is either set or adjusted using the {\it{clock\_set\_time}}
or {\it{clock\_adjust\_time}} call, respectively.
The clock is set if the time difference is equal or greater than
the offset threshold value.
The NTP specification suggests 0.125~seconds as the default~\cite{rfc5905}.
Because the designed call for setting the time, described in section~\ref{sec:design-interface},
can set the time only within a resolution of one second,
the threshold value must be at least one second.
