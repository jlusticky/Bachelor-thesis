%=========================================================================
% (c) 2011, 2012 Josef Lusticky

\section{Contiki NTP client}
The client application itself is a Contiki process,
that will use the designed operating system interface from the previous sections
and the uIP communication stack.

The client should be able to use both NTP communication modes,
the NTP broadcast mode and the NTP unicast mode.
The NTP broadcast communication mode is intended particularly for energy or
even more memory constrained clients.
If the client will use only the broadcast mode, the structures and code
related to the unicast mode should not be included in the resulting program.

%TODO


The NTP client fills and checks only the seconds part of the NTP timestamp,
because the conversion to the NTP format would increase the interval
between the timestamp determination and the packet dispatch.

After the filled NTP packet is sent, the client schedules
sending of the next NTP packet in $2^{\tau}$ seconds
using the event timer library.
In NTPv4, $\tau$ ranges from 4, resulting in NTP poll interval of 16 seconds,
to 17, resulting in NTP poll interval of 36 hours.
However, the event timer library imposes a limit on the scheduled time.
This limit is platform specific and depends on the {\it{CLOCK\_SECOND}} value,
e.g. the $\tau$ value can not be greater than 8 on AVR Raven assuming 128 interrupts per second.
Upon scheduling the event timer, the client process yields
and another process can be run.
The client process is later invoked either by the uIP stack event
announcing the server response
or by the event timer in case no server response arrived.
The event timer therefore effectively solves
the possible packet loss problem described in section~\ref{sec:analysis-application}.


%%
The packet loss problem was described in section~\ref{sec:analysis-application}.
However, packet loss is not a matter for NTP if using either broadcast or unicast mode.
In broadcast mode, a lost server packet causes no setting or adjusting the client's system time.
The client simply waits without disruption for the next NTP broadcast message.
If the client needs to figure out it's local clock offset at the moment,
it can simply query the server using the NTP unicast mode.

%%%TODO


When the server response arrives,
the destination timestamp determination is one of the first tasks the client does.
After that, the client makes packet sanity tests, including
checking whether the response is from the synchronised server.

A determination of the NTP communication mode follows.
In the unicast mode, the Originate timestamp is compared with the stored sent timestamp.

The received packet is considered bogus in case of mismatch and further processing is stopped.
Otherwise, the NTP timestamps are converted to the local timestamp format and
the local clock offset is computed as described in section~\ref{sec:ntp-algorithms}.
After the local clock offset is computed,
the stored transmitted timestamp is immediately set to zero
to protect against a replay of the last transmitted packet.

In broadcast mode, the received packet is always considered correct
and the local clock offset is computed as the difference between the local stored timestamp
and the received Transmit timestamp.
The local clock offset determined from the broadcast mode
is influenced by the network propagation delay and therefore less accurate.

The NTP client could exchange several packets with the server to calibrate the propagation delay.
But since local variables can not be reused in the Contiki process when the process yields,
this would cause either an extra memory overhead or a complicate client design.


Dynamic increasing or decreasing the client's Poll interval in response to
Kiss-o'-Death packets, described in section~\ref{sec:ntp-network}, is not implemented.
The configuration instead assumes, that an exhausted NTP server rather drops the incoming
client's packet than sending the response with a KoD code.



Due to a different origin of the Unix and NTP epoch,
the number of seconds between NTP and Unix epoch,
can be simply subtracted from the seconds part of the NTP timestamp.
But the conversion from the fraction part of the long 64-bit NTP timestamp to nanoseconds,
used in the local timestamp structure,
is one of the most problematic tasks for memory constrained systems.
An accurate conversion requires either floating point operations or operations including 64-bit numbers.
The conversion is given by
$nsec = fractionl \times 10^9 \div 2^{32}$, where $nsec$ is the nanoseconds part of the local timestamp
and $fractionl$ is the fraction part of the long 64-bit NTP timestamp.
Since there is no native hardware support for floating point nor 64-bit arithmetic operations,
the compiler supplies these operations in form of library, called {\it{libgcc}} in case of the GCC compiler,
which causes a significantly bigger resulting binary file.
The greatest common divisor of $10^9$ and $2^{32}$ is $2^9$,
so in fact, a relatively simple multiplication of $fractionl$ by $\frac{5^9}{2^{23}}$ must be computed.
This can be computed using sequential divisions and multiplications, which
in turn can be done on 32 bits using shifts and additions~\cite{c99}.

$\frac{10^9}{2^{32}} = \frac{2^9 \times 5^9}{2^9 \times 2^{23}} =
\frac{5^9}{2^{23}} = \frac{5}{2^3} \times \frac{5}{2^3} \times \frac{5}{2^3} \times \frac{5^2}{2^3}
\times \frac{5}{2^3}  \times \frac{5}{2^3} \times \frac{5^2}{2^3} \times \frac{1}{2^2}$
%the last must be done, 2 and 5 are both prime nubmers!


After the timestamps were converted, the local clock offset is computed
as given in section~\ref{sec:ntp-algorithms}.
Depending on the absolute value of the local clock offset,
the system time is either set or adjusted using the {\it{clock\_set\_time}}
or {\it{clock\_adjust\_time}} call, respectively.
The clock is set if the time difference is equal or greater than
the threshold value, which is 3 seconds by default. %!TODO
The reference NTP implementation uses 125 ms~\cite{rfc5905}
It has been the Internet
experience that the need to change the system time in increments
greater than +-128 ms is extremely rare and is usually associated
with a hardware or software malfunction or system reboot~\cite{rfc1589}.
